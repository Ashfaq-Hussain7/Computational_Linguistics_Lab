{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuAS2KIypZAdZJ2/vwfkcc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashfaq-Hussain7/Computational_Linguistics_Lab/blob/main/Computational_Linguistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1**\n"
      ],
      "metadata": {
        "id": "fcUfBoznjzjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a program that reads a paragraph and:\n",
        "* Tokenizes the text into words.\n",
        "* Removes punctuation and converts all words to lowercase.\n",
        "* Performs stemming and lemmatization."
      ],
      "metadata": {
        "id": "AI_E4N4fvWIJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yda-RcMTjxwU"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "ONrFvA-tnjf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paragraph\n",
        "paragraph = \"\"\"Natural Language Processing (NLP) is a fascinating field. It involves teaching computers to understand human language!\"\"\"\n"
      ],
      "metadata": {
        "id": "oJR3gESCnmsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(paragraph)\n",
        "\n",
        "# Remove punctuation and convert to lowercase\n",
        "words = [word.lower() for word in tokens if word.isalnum()]\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Perform lemmatization\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"
      ],
      "metadata": {
        "id": "wNVjHRi4nrLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Words:\\n\",paragraph)\n",
        "print(\"\\nLowerCased Words:\\n\", words)\n",
        "print(\"\\nStemmed Words:\\n\", stemmed_words)\n",
        "print(\"\\nLemmatized Words:\\n\", lemmatized_words)"
      ],
      "metadata": {
        "id": "HY5VnLx4oUoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HpP0OCu0rKWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2**"
      ],
      "metadata": {
        "id": "7ww-iqTkqtW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a program that reads a paragraph and:\n",
        "* Extracts digits from it.\n",
        "* Extracts phone numbers from it."
      ],
      "metadata": {
        "id": "f1qNGyjvvc3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "f6_gkslzrJmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Input paragraph\n",
        "paragraph = input(\"Enter a paragraph: \") #You can reach me at 9876543210 or 9123456789. I also have an office contact number 04842345678. The pin code is 682022 and my ID is 12345."
      ],
      "metadata": {
        "id": "d5YdlYjIrPxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all digits (individual numbers)\n",
        "digits = re.findall(r'\\d', paragraph)"
      ],
      "metadata": {
        "id": "NV5inFTgrZsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract phone numbers (assuming 10-digit mobile numbers and 11 for telephone numbers)\n",
        "phone_numbers = re.findall(r'\\b\\d{10,11}\\b', paragraph)\n"
      ],
      "metadata": {
        "id": "Zo2YRPHaraT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print\n",
        "print(\"The Original Paragraph: \", paragraph)\n",
        "if digits:\n",
        "    print(\"\\nExtracted Digits:\\n\", digits)\n",
        "else:\n",
        "    print(\"\\nNo digits found in the paragraph.\")\n",
        "\n",
        "if phone_numbers:\n",
        "    print(\"\\nExtracted Phone Numbers:\\n\", phone_numbers)\n",
        "else:\n",
        "    print(\"\\nNo phone numbers found in the paragraph.\")"
      ],
      "metadata": {
        "id": "eoj5cKkTsKgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G4Fi3bwi17ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 3**"
      ],
      "metadata": {
        "id": "vL7jb0MWvLnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a simple rule based text tokenizer for the english language using regex. Your tokenixer should consider punctuations and special symblos as separate tokens. Contractions like (isn't) should be regarded as 2 token isn and t. Also identify abbrevations(USA) and internal hyphonation (eg: ice-cream) as single token."
      ],
      "metadata": {
        "id": "DY3cI2vQvn2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "AWp-OHwJwffr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_tokenizer(text):\n",
        "    # Split contractions: isn't -> isn t\n",
        "    text = re.sub(r\"(\\w+)'(\\w+)\", r\"\\1 \\2\", text)\n",
        "\n",
        "    # Regex pattern:\n",
        "    #  - Abbreviations: USA\n",
        "    #  - Hyphenated words: ice-cream\n",
        "    #  - Words/numbers\n",
        "    #  - Punctuation/special symbols\n",
        "    pattern = r\"[A-Z]{2,}|\\w+(?:-\\w+)*|[^\\w\\s]\"\n",
        "\n",
        "    # Extract tokens\n",
        "    return re.findall(pattern, text)"
      ],
      "metadata": {
        "id": "mVVynd6r1kX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input\n",
        "paragraph = \"Dr. Smith isn't available today; he's in the USA enjoying an ice-cream at 3:00 p.m.!\"\n",
        "tokens = simple_tokenizer(paragraph)\n"
      ],
      "metadata": {
        "id": "-IEjJrDL1kxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output\n",
        "print(\"Original Text:\\n\", paragraph)\n",
        "print(\"\\nTokens:\\n\", tokens)"
      ],
      "metadata": {
        "id": "PIdjmwlk1vQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zz3vrzWy4bCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4**"
      ],
      "metadata": {
        "id": "m52ZB4kA4b3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a program or a script that corrects a single non-word spelling error based on the noisy channel model.  \n",
        "\n",
        "\n",
        "\n",
        "Instructions:\n",
        "\n",
        "Corpus and Dictionary:\n",
        "\n",
        "Create a simple dictionary (V) of at least 5 common words.\n",
        "\n",
        "Error Simulation and Candidate Generation:\n",
        "\n",
        "Choose one word from your dictionary (e.g., \"word\") and introduce a single-character error (e.g., a substitution or transposition) to create a non-word misspelling (e.g., \"wrod\").\n",
        "\n",
        "Manually generate a candidate set of at least 3 plausible corrections for your misspelled word. These candidates must be real words from your dictionary.\n",
        "\n",
        "Calculating Probabilities:\n",
        "\n",
        "For each candidate word w and your misspelled word s, you need to calculate P(s‚à£w). You will need to make some assumptions for this part, as you don't have a real confusion matrix.\n",
        "\n",
        "Assumption: For a single-edit error, you can assign a fixed, small probability value (e.g., 0.001) for the channel model, representing the likelihood of that specific error type.\n",
        "\n",
        "Finding the Best Correction:\n",
        "\n",
        "Using the formula w=argmax w‚ààV P(s‚à£w)P(w), calculate the final score for each candidate in your set.\n",
        "\n",
        "\n",
        "Your program should output the candidate with the highest score as the corrected word."
      ],
      "metadata": {
        "id": "JSPvm47C4ubQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Noisy Channel Spell Corrector\n",
        "\n",
        "import editdistance\n",
        "\n",
        "# Step 1: Dictionary\n",
        "dictionary = [\"world\", \"word\", \"work\", \"worm\", \"wood\"]\n",
        "\n",
        "# Step 2: Simulated error (target misspelled word)\n",
        "misspelled = \"wrod\"   # from \"word\"\n",
        "\n",
        "# Step 3: Candidate corrections (subset of dictionary)\n",
        "candidates = [\"word\", \"world\", \"work\"]\n",
        "\n",
        "# Step 4: Assumptions for probabilities\n",
        "# Channel model: P(s|w)\n",
        "def channel_probability(s, w):\n",
        "    d = editdistance.eval(s, w)  # Levenshtein distance\n",
        "    if d == 0:\n",
        "        return 0.9\n",
        "    elif d == 1:\n",
        "        return 0.01\n",
        "    elif d == 2:\n",
        "        return 0.001\n",
        "    else:\n",
        "        return 1e-6\n",
        "\n",
        "# Step 4b: Assumed prior probabilities (must sum to 1)\n",
        "priors = {\n",
        "    \"word\": 0.4,\n",
        "    \"world\": 0.3,\n",
        "    \"work\": 0.2,\n",
        "    \"worm\": 0.05,\n",
        "    \"wood\": 0.05\n",
        "}\n",
        "\n",
        "# Step 5: Compute scores for candidates\n",
        "scores = {}\n",
        "for w in candidates:\n",
        "    P_s_given_w = channel_probability(misspelled, w)\n",
        "    P_w = priors[w]   # use assumed prior\n",
        "    score = P_s_given_w * P_w\n",
        "    scores[w] = score\n",
        "\n",
        "# Step 6: Find best candidate\n",
        "best_word = max(scores, key=scores.get)\n",
        "\n",
        "# Output\n",
        "print(\"Misspelled Word:\", misspelled)\n",
        "print(\"Candidate Probabilities and Scores:\")\n",
        "for w, score in scores.items():\n",
        "    print(f\"  {w}: P(w)={priors[w]}, P(s|w)={channel_probability(misspelled,w)}, Score={score}\")\n",
        "print(\"Corrected Word:\", best_word)\n"
      ],
      "metadata": {
        "id": "oH1TrTaVFJtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MYKZG1tmEK9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5**"
      ],
      "metadata": {
        "id": "cCupUkP9EJ9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a text classifier for sentiment analysis using naive bayes theorem."
      ],
      "metadata": {
        "id": "1uOfp7fWFZnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes Sentiment Analysis Classifier\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "rF2e5repFeXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "\n",
        "# Try alternative encodings\n",
        "df = pd.read_csv(\"train.csv\", encoding='ISO-8859-1')  # or encoding='latin1'\n",
        "print(df.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "BS4UdPLdF3iD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only relevant columns\n",
        "df = df[['text', 'sentiment']].dropna()\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nSample Data:\")\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "fIAiseUUF3fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train & test\n",
        "X = df['text']\n",
        "y = df['sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "KaUzfd9KF3cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words='english',\n",
        "    lowercase=True,\n",
        "    ngram_range=(1, 2),  # use unigrams and bigrams\n",
        "    max_features=5000    # limit feature size for speed\n",
        ")\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "fBy28AzWF_kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "60HLh2J1F_gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate performance\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"‚úÖ Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
      ],
      "metadata": {
        "id": "ecw7FKePGEp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on new examples\n",
        "new_tweets = [\n",
        "    \"I absolutely loved the new feature update!\",\n",
        "    \"This service is okay, nothing special.\",\n",
        "    \"Worst experience ever. Totally disappointed.\"\n",
        "]\n",
        "\n",
        "new_vec = vectorizer.transform(new_tweets)\n",
        "preds = model.predict(new_vec)\n",
        "probs = model.predict_proba(new_vec)\n",
        "\n",
        "print(\"\\nüîÆ Predictions on new tweets:\")\n",
        "for text, pred, prob in zip(new_tweets, preds, probs):\n",
        "    print(f\"\\n'{text}' ‚Üí {pred} (probabilities: {dict(zip(model.classes_, prob.round(3)))})\")"
      ],
      "metadata": {
        "id": "hpBT-V9rGEnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 6**"
      ],
      "metadata": {
        "id": "IilLb8RnEBlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform POS tagging for an Indian Language(eg: Hindi or Malayalam) using a pretrained model or dataset.\n",
        "Task:\n",
        "* Load a small set of sentences in your target language.\n",
        "* Use librarires such as Stanza, IndicNLP, or iNLTK to tag parts of speech.\n",
        "* Identify common tag types and compare with English tags.\n",
        "* Discuss the changes of POS tagging in morphologically rich languages."
      ],
      "metadata": {
        "id": "HvTxB4CpEIrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "id": "mYgbOURcIKfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza --quiet\n"
      ],
      "metadata": {
        "id": "LwDwHmOVE7R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza"
      ],
      "metadata": {
        "id": "duX5Hc1GJfx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Hindi and English models\n",
        "stanza.download('hi')\n",
        "stanza.download('en')"
      ],
      "metadata": {
        "id": "uKQZPIUxHO4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize NLP pipelines\n",
        "nlp_hi = stanza.Pipeline('hi', processors='tokenize,pos', use_gpu=False)\n",
        "nlp_en = stanza.Pipeline('en', processors='tokenize,pos', use_gpu=False)"
      ],
      "metadata": {
        "id": "L2sujMxiKNyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hindi sentences\n",
        "hindi_sentences = [\n",
        "    \"‡§Æ‡•à‡§Ç ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å‡•§\",\n",
        "    \"‡§µ‡§π ‡§è‡§ï ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§≤‡§°‡§º‡§ï‡•Ä ‡§π‡•à‡•§\",\n",
        "    \"‡§≠‡§æ‡§∞‡§§ ‡§è‡§ï ‡§∏‡•Å‡§Ç‡§¶‡§∞ ‡§¶‡•á‡§∂ ‡§π‡•à‡•§\"\n",
        "]\n",
        "\n",
        "print(\"üå∏ POS Tagging in Hindi (Stanza):\\n\")\n",
        "for sentence in hindi_sentences:\n",
        "    doc = nlp_hi(sentence)\n",
        "    for sent in doc.sentences:\n",
        "        print(f\"üîπ Sentence: {sentence}\")\n",
        "        for word in sent.words:\n",
        "            print(f\"{word.text:10} ‚Üí {word.upos}\")\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "XEzegZZJKRMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# English sentences (for comparison)\n",
        "\n",
        "english_sentences = [\n",
        "    \"I am going to school.\",\n",
        "    \"She is a good girl.\",\n",
        "    \"India is a beautiful country.\"\n",
        "]\n",
        "\n",
        "print(\"\\nüá¨üáß POS Tagging in English (Stanza):\\n\")\n",
        "for sentence in english_sentences:\n",
        "    doc = nlp_en(sentence)\n",
        "    for sent in doc.sentences:\n",
        "        print(f\"üîπ Sentence: {sentence}\")\n",
        "        for word in sent.words:\n",
        "            print(f\"{word.text:10} ‚Üí {word.upos}\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "cdFgXNOrKWx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tF9ByD9q0c2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ko9JT9l0czJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7**"
      ],
      "metadata": {
        "id": "5swr1GLGP5Xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a program to compute translation probabilities P(f/e) and P(e/F) from a parallel corpus. Consider a corpus of english and malayalam containing 5 sentences (manually construct it)"
      ],
      "metadata": {
        "id": "-CnZbU73Qsbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Translation Probabilities\n",
        "\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "cLnJFWzcQLDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parallel corpus (5 sentences)\n",
        "english_sentences = [\n",
        "    \"I love apples\",\n",
        "    \"I eat apples\",\n",
        "    \"She loves mango\",\n",
        "    \"He eats rice\",\n",
        "    \"I buy rice\"\n",
        "]\n",
        "\n",
        "malayalam_sentences = [\n",
        "    \"‡¥é‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥Ü‡¥™‡µç‡¥™‡¥ø‡µæ ‡¥á‡¥∑‡µç‡¥ü‡¥Æ‡¥æ‡¥£‡µç\",\n",
        "    \"‡¥û‡¥æ‡µª ‡¥Ü‡¥™‡µç‡¥™‡¥ø‡µæ ‡¥ï‡¥¥‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ\",\n",
        "    \"‡¥Ö‡¥µ‡µæ ‡¥Æ‡¥æ‡¥ô‡µç‡¥ô ‡¥á‡¥∑‡µç‡¥ü‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥®‡µç‡¥®‡µÅ\",\n",
        "    \"‡¥Ö‡¥µ‡µª ‡¥Ö‡¥∞‡¥ø ‡¥ï‡¥¥‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ\",\n",
        "    \"‡¥û‡¥æ‡µª ‡¥Ö‡¥∞‡¥ø ‡¥µ‡¥æ‡¥ô‡µç‡¥ô‡µÅ‡¥®‡µç‡¥®‡µÅ\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "25zUGSv0QLAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize by space\n",
        "corpus = list(zip(\n",
        "    [s.lower().split() for s in english_sentences],\n",
        "    [s.split() for s in malayalam_sentences]\n",
        "))\n"
      ],
      "metadata": {
        "id": "d31CE3j6RfNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count co-occurrences\n",
        "count_ef = defaultdict(int)  # (e,f) joint count\n",
        "count_e = defaultdict(int)\n",
        "count_f = defaultdict(int)\n",
        "\n",
        "for en_tokens, ml_tokens in corpus:\n",
        "    for e in en_tokens:\n",
        "        for f in ml_tokens:\n",
        "            count_ef[(e, f)] += 1\n",
        "            count_e[e] += 1\n",
        "            count_f[f] += 1"
      ],
      "metadata": {
        "id": "U2j8PR6kRfJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute P(f | e) and P(e | f)\n",
        "P_f_given_e = {}\n",
        "P_e_given_f = {}\n",
        "\n",
        "for (e,f), c in count_ef.items():\n",
        "    P_f_given_e[(e,f)] = c / count_e[e]\n",
        "    P_e_given_f[(e,f)] = c / count_f[f]\n"
      ],
      "metadata": {
        "id": "Z88H78jhRfHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Results\n",
        "\n",
        "print(\"=== Translation Probability P(f | e) ===\")\n",
        "for pair, prob in P_f_given_e.items():\n",
        "    print(f\"{pair}: {prob:.4f}\")\n",
        "\n",
        "print(\"\\n=== Translation Probability P(e | f) ===\")\n",
        "for pair, prob in P_e_given_f.items():\n",
        "    print(f\"{pair}: {prob:.4f}\")"
      ],
      "metadata": {
        "id": "DUuQ5qIrRfEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDFR2x-RRfCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8**"
      ],
      "metadata": {
        "id": "znDOCNjfRtFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a word clustering using word embedding (Word2vec) and plot clusters using PCA."
      ],
      "metadata": {
        "id": "VEGvwGVQS0hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "v5-Ic3cOW0f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "R_Dka-bcS1Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Sample Corpus\n",
        "sentences = [\n",
        "    [\"king\", \"queen\", \"monarch\", \"royal\"],\n",
        "    [\"man\", \"woman\", \"child\"],\n",
        "    [\"apple\", \"banana\", \"fruit\", \"food\"],\n",
        "    [\"paris\", \"france\", \"city\", \"europe\"],\n",
        "    [\"dog\", \"cat\", \"animal\", \"pet\"],\n",
        "]\n"
      ],
      "metadata": {
        "id": "ChUEzsllWfeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Train Word2Vec\n",
        "model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=50,   # embedding dimensions\n",
        "    window=3,\n",
        "    min_count=1,\n",
        "    sg=1              # skip-gram\n",
        ")\n"
      ],
      "metadata": {
        "id": "69oOxd8hWhaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Extract Embeddings\n",
        "words = list(model.wv.index_to_key)\n",
        "vectors = model.wv[words]"
      ],
      "metadata": {
        "id": "DNXcrCdWWhXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Cluster with KMeans\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "labels = kmeans.fit_predict(vectors)\n"
      ],
      "metadata": {
        "id": "WYtWTyxmWhVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Reduce to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(vectors)"
      ],
      "metadata": {
        "id": "Yx4hnOhxWuCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "for i, word in enumerate(words):\n",
        "    x, y = reduced[i]\n",
        "    plt.scatter(x, y, c=f\"C{labels[i]}\")\n",
        "    plt.text(x+0.02, y+0.02, word)\n",
        "\n",
        "plt.title(\"Word Clusters (Word2Vec + PCA)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MqBXr1u4Wvax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VA1x09X5aLz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 9**"
      ],
      "metadata": {
        "id": "CkWOMqd2aLQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design and implement a Finite State Automata(FSA) that accepts English plural nouns ending with the character ‚Äòy‚Äô, e.g. boys, toys, ponies, skies, and puppies but not boies or toies or ponys. (Hint: Words that end with a vowel followed by ‚Äòy‚Äô are appended with ‚Äòs' and will not be replaced with ‚Äúies‚Äù in their plural form)."
      ],
      "metadata": {
        "id": "P3r1ma1oaPuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finite State Automata for English plural nouns ending with 'y'\n",
        "\n",
        "def is_valid_plural(word):\n",
        "    vowels = set(\"aeiou\")\n",
        "\n",
        "    # A plural must have at least 3 characters\n",
        "    if len(word) < 3:\n",
        "        return False\n",
        "\n",
        "    # =========================\n",
        "    # CASE 1: vowel + y + s\n",
        "    # Example: toy ‚Üí toys\n",
        "    # =========================\n",
        "    if word.endswith(\"ys\"):\n",
        "        # character before \"ys\"\n",
        "        prev = word[-3]\n",
        "        return prev in vowels\n",
        "\n",
        "    # =========================\n",
        "    # CASE 2: consonant + ies\n",
        "    # Example: puppy ‚Üí puppies\n",
        "    # =========================\n",
        "    if word.endswith(\"ies\"):\n",
        "        # character before the 'i'\n",
        "        prev = word[-4]\n",
        "        return prev not in vowels\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# -------------------------------------------\n",
        "# Test the automaton with sample plural words\n",
        "# -------------------------------------------\n",
        "\n",
        "test_words = [\n",
        "    \"boys\", \"toys\", \"ponies\", \"skies\", \"puppies\",\n",
        "    \"boies\", \"ponys\", \"toies\", \"dogs\", \"cat\"\n",
        "]\n",
        "\n",
        "print(\"=== FSA results ===\")\n",
        "for w in test_words:\n",
        "    print(f\"{w:10} ‚Üí {is_valid_plural(w)}\")\n",
        "\n",
        "\n",
        "# Optional: User input\n",
        "while True:\n",
        "    word = input(\"\\nEnter a plural word (or 'exit'): \").strip().lower()\n",
        "    if word == \"exit\":\n",
        "        break\n",
        "    print(\"Valid plural?\" , is_valid_plural(word))\n"
      ],
      "metadata": {
        "id": "Mnv1H3FZaPTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i9Pyt9M0aKz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Clustering**"
      ],
      "metadata": {
        "id": "ZG8HfJc-QMZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# text data\n",
        "documents = [\n",
        "    \"Text clustering is a task in NLP.\",\n",
        "    \"NLP involves text preprocessing and feature extraction.\",\n",
        "    \"K-Means is a popular clustering algorithm.\",\n",
        "    \"Evaluation of clustering can be done using various metrics.\",\n",
        "    \"Word embeddings capture semantic meaning.\",\n",
        "    \"Hierarchical clustering builds a hierarchy of clusters.\",\n",
        "    \"DBSCAN identifies clusters based on density.\",\n",
        "    \"Clustering is used in document organization.\",\n",
        "    \"Preprocessing includes tokenization and stop word removal.\",\n",
        "    \"Dimensionality reduction helps in visualization.\"\n",
        "]\n",
        "\n",
        "# Text Preprocessing and Feature Extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Dimensionality Reduction (optional, for visualization)\n",
        "pca = PCA(n_components=2)\n",
        "X_reduced = pca.fit_transform(X.toarray())\n",
        "\n",
        "# Clustering using K-Means\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "clusters = kmeans.fit_predict(X)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 7))\n",
        "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=clusters, cmap='viridis')\n",
        "plt.title(\"Text Clustering Visualization\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "\n",
        "# Adding cluster centers to the plot\n",
        "centers = kmeans.cluster_centers_\n",
        "centers_reduced = pca.transform(centers)\n",
        "plt.scatter(centers_reduced[:, 0], centers_reduced[:, 1], c='red', s=200, alpha=0.75, marker='X')\n",
        "\n",
        "# Adding labels to the plot\n",
        "for i, txt in enumerate(documents):\n",
        "    plt.annotate(txt, (X_reduced[i, 0], X_reduced[i, 1]), fontsize=9, alpha=0.75)\n",
        "\n",
        "plt.colorbar(scatter, label='Cluster Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9IUGggmz0cwt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}